---
title: Multi-modal fusion by meta-initialization
abstract: When experience is scarce, models may have insufficient information to
  adapt to a new task. In this case, auxiliary information - such as a textual
  description of the task - can enable improved task inference and adaptation.
  In this work, we propose an extension to the Model-Agnostic Meta-Learning
  algorithm (MAML), which allows the model to adapt using auxiliary information
  as well as task experience. Our method, Fusion by Meta-Initialization (FuMI),
  conditions the model initialization on auxiliary information using a
  hypernetwork, rather than learning a single, task-agnostic initialization.
  Furthermore, motivated by the shortcomings of existing multi-modal few-shot
  learning benchmarks, we constructed iNat-Anim - a large-scale image
  classification dataset with succinct and visually pertinent textual class
  descriptions. On iNat-Anim, FuMI significantly outperforms uni-modal baselines
  such as MAML in the few-shot regime.
slides: ""
url_pdf: https://arxiv.org/abs/2210.04843
publication_types:
  - "3"
authors:
  - Matthew Jackson
  - Shreshth Malik
  - admin
  - Yousuf Mohamed-Ahmed
publication: arXiv preprint
featured: false
tags: []
projects: []
summary: ""
url_dataset: ""
url_project: ""
publication_short: ""
url_source: ""
url_video: ""
date: 2022-10-10T00:00:00.000Z
url_slides: ""
links: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
publishDate: 2017-01-01T00:00:00.000Z
url_poster: ""
url_code: ""
doi: ""
---
